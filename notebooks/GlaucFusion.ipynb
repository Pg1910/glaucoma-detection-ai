{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0YvDrfi2vQGo","executionInfo":{"status":"ok","timestamp":1752005842980,"user_tz":-330,"elapsed":35061,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"87aeba5f-0a56-482b-f653-e46b2b67e741"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## data preprocessing of the input images and applying both the filters(blur + cdr threshold)"],"metadata":{"id":"YiDrztFzae03"}},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision import transforms\n","from PIL import Image\n","from segmentation_models_pytorch import Unet\n","\n","# ------------------------\n","# CONFIGURATION\n","# ------------------------\n","MODEL_PATH = \"/content/drive/MyDrive/glaucoma_detection/cdr_src/unet_efficientnetb4_drishti.pth\"\n","TEST_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets\"\n","OUTPUT_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/segmentation_overlays_unet/test_metadata.csv\"\n","\n","RESIZE_DIM = (512, 512)\n","BLUR_THRESHOLD = 15.0\n","CDR_LOWER = 0.1\n","CDR_UPPER = 1.2\n","CHUNK_SIZE = 25\n","\n","device = torch.device(\"cpu\")\n","\n","# ------------------------\n","# LOAD SMP UNET MODEL\n","# ------------------------\n","print(\"📥 Loading SMP U-Net EfficientNet-B4 on CPU...\")\n","\n","model = Unet(\n","    encoder_name=\"efficientnet-b4\",\n","    encoder_weights=None,  # ✅ Must be None to match saved weights\n","    in_channels=3,\n","    classes=2\n",").to(device)\n","\n","state_dict = torch.load(MODEL_PATH, map_location=device)\n","model.load_state_dict(state_dict)\n","model.eval()\n","\n","print(\"✅ Model loaded successfully\")\n","\n","# ------------------------\n","# IMAGE TRANSFORMS\n","# ------------------------\n","preprocess = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(RESIZE_DIM),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet mean\n","                         [0.229, 0.224, 0.225])  # ImageNet std\n","])\n","\n","def compute_blur_score(image):\n","    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    return cv2.Laplacian(gray, cv2.CV_64F).var()\n","\n","def compute_vertical_extent(mask):\n","    vertical_indices = np.any(mask, axis=1)\n","    return np.sum(vertical_indices)\n","\n","@torch.no_grad()\n","def compute_cdr(image_np):\n","    input_tensor = preprocess(image_np).unsqueeze(0).to(device)\n","    output = model(input_tensor)\n","    probs = torch.sigmoid(output).squeeze().cpu().numpy()  # [2, H, W]\n","\n","    disc_mask = probs[0] > 0.5\n","    cup_mask = probs[1] > 0.5\n","\n","    disc_vert = compute_vertical_extent(disc_mask)\n","    cup_vert = compute_vertical_extent(cup_mask)\n","\n","    if disc_vert == 0:\n","        return -1\n","\n","    return cup_vert / disc_vert\n","\n","# ------------------------\n","# CHUNKED PROCESSING\n","# ------------------------\n","def process_in_chunks(image_list, label, output_df, folder_path):\n","    for i in range(0, len(image_list), CHUNK_SIZE):\n","        chunk = image_list[i:i+CHUNK_SIZE]\n","        chunk_metadata = []\n","\n","        for img_name in tqdm(chunk, desc=f\"Chunk [{i}-{i+len(chunk)}]\"):\n","            img_path = os.path.join(folder_path, img_name)\n","            try:\n","                img = cv2.imread(img_path)\n","                if img is None:\n","                    print(f\"⚠️ Skipping unreadable: {img_path}\")\n","                    continue\n","\n","                img_resized = cv2.resize(img, RESIZE_DIM)\n","                blur_score = compute_blur_score(img_resized)\n","                vCDR = compute_cdr(img_resized)\n","\n","                passed = (\n","                    blur_score > BLUR_THRESHOLD and\n","                    CDR_LOWER <= vCDR <= CDR_UPPER\n","                )\n","\n","                chunk_metadata.append({\n","                    \"image_path\": img_path,\n","                    \"label\": label,\n","                    \"vCDR\": round(vCDR, 3),\n","                    \"blur_score\": round(blur_score, 2),\n","                    \"passed_filtering\": passed\n","                })\n","\n","            except Exception as e:\n","                print(f\"❌ Error processing {img_name}: {e}\")\n","\n","        chunk_df = pd.DataFrame(chunk_metadata)\n","        output_df = pd.concat([output_df, chunk_df], ignore_index=True)\n","        output_df.to_csv(OUTPUT_CSV, index=False)\n","\n","    return output_df\n","\n","# ------------------------\n","# MAIN LOOP OVER FOLDERS\n","# ------------------------\n","output_df = pd.DataFrame()\n","\n","for label_dir in [\"GON+\", \"GON-\"]:\n","    label = 1 if label_dir == \"GON+\" else 0\n","    folder_path = os.path.join(TEST_ROOT, label_dir)\n","    image_list = os.listdir(folder_path)\n","\n","    print(f\"\\n🔄 Processing {label_dir} with {len(image_list)} images\")\n","    output_df = process_in_chunks(image_list, label, output_df, folder_path)\n","\n","print(f\"\\n✅ All chunks processed. Final metadata saved to: {OUTPUT_CSV}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUcOwRKBxU_W","executionInfo":{"status":"ok","timestamp":1751976081314,"user_tz":-330,"elapsed":635093,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"99a3dcef-6d74-4e4a-f2e1-a2f73b8b12ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📥 Loading SMP U-Net EfficientNet-B4 on CPU...\n","✅ Model loaded successfully\n","\n","🔄 Processing GON+ with 32 images\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [0-25]: 100%|██████████| 25/25 [00:42<00:00,  1.71s/it]\n","Chunk [25-32]: 100%|██████████| 7/7 [00:11<00:00,  1.70s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n","🔄 Processing GON- with 360 images\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [0-25]:   8%|▊         | 2/25 [00:02<00:33,  1.44s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0166.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\rChunk [0-25]:  16%|█▌        | 4/25 [00:05<00:26,  1.28s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0194.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [0-25]: 100%|██████████| 25/25 [00:37<00:00,  1.51s/it]\n","Chunk [25-50]: 100%|██████████| 25/25 [00:39<00:00,  1.58s/it]\n","Chunk [50-75]: 100%|██████████| 25/25 [00:47<00:00,  1.89s/it]\n","Chunk [75-100]: 100%|██████████| 25/25 [00:40<00:00,  1.63s/it]\n","Chunk [100-125]: 100%|██████████| 25/25 [00:40<00:00,  1.62s/it]\n","Chunk [125-150]: 100%|██████████| 25/25 [00:40<00:00,  1.60s/it]\n","Chunk [150-175]:  56%|█████▌    | 14/25 [00:21<00:16,  1.49s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0060.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\rChunk [150-175]:  64%|██████▍   | 16/25 [00:24<00:11,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0136.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [150-175]: 100%|██████████| 25/25 [00:36<00:00,  1.47s/it]\n","Chunk [175-200]:  40%|████      | 10/25 [00:16<00:26,  1.74s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0178.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [175-200]:  64%|██████▍   | 16/25 [00:24<00:12,  1.44s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0216.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [175-200]:  80%|████████  | 20/25 [00:29<00:07,  1.41s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0106.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Chunk [175-200]: 100%|██████████| 25/25 [00:36<00:00,  1.44s/it]\n","Chunk [200-225]: 100%|██████████| 25/25 [00:42<00:00,  1.69s/it]\n","Chunk [225-250]: 100%|██████████| 25/25 [00:40<00:00,  1.63s/it]\n","Chunk [250-275]: 100%|██████████| 25/25 [00:41<00:00,  1.66s/it]\n","Chunk [275-300]: 100%|██████████| 25/25 [00:39<00:00,  1.60s/it]\n","Chunk [300-325]: 100%|██████████| 25/25 [00:40<00:00,  1.62s/it]\n","Chunk [325-350]: 100%|██████████| 25/25 [00:40<00:00,  1.63s/it]\n","Chunk [350-360]: 100%|██████████| 10/10 [00:16<00:00,  1.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ All chunks processed. Final metadata saved to: test_metadata.csv\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## segmentation overlays created using unet to pass forward to eomt"],"metadata":{"id":"lBe0OKjSaJoc"}},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from torchvision import transforms\n","from segmentation_models_pytorch import Unet\n","from PIL import Image\n","\n","# ------------------------\n","# CONFIGURATION\n","# ------------------------\n","MODEL_PATH = \"/content/drive/MyDrive/glaucoma_detection/cdr_src/unet_efficientnetb4_drishti.pth\"\n","TEST_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets\"\n","OUTPUT_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets/segmentation_overlays_unet\"\n","RESIZE_DIM = (512, 512)\n","\n","device = torch.device(\"cpu\")\n","\n","# ------------------------\n","# LOAD SMP UNET MODEL\n","# ------------------------\n","model = Unet(\n","    encoder_name=\"efficientnet-b4\",\n","    encoder_weights=None,\n","    in_channels=3,\n","    classes=2\n",").to(device)\n","\n","state_dict = torch.load(MODEL_PATH, map_location=device)\n","model.load_state_dict(state_dict)\n","model.eval()\n","\n","# ------------------------\n","# TRANSFORMS\n","# ------------------------\n","preprocess = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(RESIZE_DIM),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","@torch.no_grad()\n","def predict_masks(img_np):\n","    input_tensor = preprocess(img_np).unsqueeze(0).to(device)\n","    output = model(input_tensor)\n","    probs = torch.sigmoid(output).squeeze().cpu().numpy()  # [2, H, W]\n","\n","    disc_mask = (probs[0] > 0.5).astype(np.uint8)\n","    cup_mask  = (probs[1] > 0.5).astype(np.uint8)\n","    return disc_mask, cup_mask\n","\n","def overlay_masks(original_img, disc_mask, cup_mask, alpha=0.4):\n","    overlay = original_img.copy()\n","    h, w, _ = overlay.shape\n","\n","    # Resize masks to original image size\n","    disc_mask_resized = cv2.resize(disc_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n","    cup_mask_resized = cv2.resize(cup_mask, (w, h), interpolation=cv2.INTER_NEAREST)\n","\n","    # Color overlay\n","    overlay[cup_mask_resized == 1] = [0, 0, 255]    # Red for cup\n","    overlay[disc_mask_resized == 1] = [0, 255, 0]   # Green for disc\n","\n","    # Blend with original\n","    blended = cv2.addWeighted(original_img, 1 - alpha, overlay, alpha, 0)\n","    return blended\n","\n","# ------------------------\n","# MAIN LOOP\n","# ------------------------\n","for label_dir in [\"GON+\", \"GON-\"]:\n","    folder_path = os.path.join(TEST_ROOT, label_dir)\n","    save_path = os.path.join(OUTPUT_ROOT, label_dir)\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    for img_name in tqdm(os.listdir(folder_path), desc=f\"Overlaying {label_dir}\"):\n","        try:\n","            img_path = os.path.join(folder_path, img_name)\n","            img = cv2.imread(img_path)\n","            if img is None:\n","                print(f\"⚠️ Skipping unreadable: {img_path}\")\n","                continue\n","\n","            img_resized = cv2.resize(img, RESIZE_DIM)\n","            disc_mask, cup_mask = predict_masks(img_resized)\n","\n","            overlay = overlay_masks(img_resized, disc_mask, cup_mask)\n","\n","            # Save\n","            save_file = os.path.join(save_path, img_name.replace(\".jpg\", \"_overlay.png\").replace(\".jpeg\", \"_overlay.png\").replace(\".png\", \"_overlay.png\"))\n","            cv2.imwrite(save_file, overlay)\n","\n","        except Exception as e:\n","            print(f\"❌ Error on {img_name}: {e}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIkIaIEoz5e-","executionInfo":{"status":"ok","timestamp":1751982711964,"user_tz":-330,"elapsed":645133,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"e6a96cf3-03b1-49d1-b807-406f2c7c9119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Overlaying GON+: 100%|██████████| 32/32 [01:03<00:00,  1.98s/it]\n","Overlaying GON-:   1%|          | 2/360 [00:03<09:24,  1.58s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0166.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\rOverlaying GON-:   1%|          | 4/360 [00:06<09:27,  1.59s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0194.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Overlaying GON-:  46%|████▌     | 164/360 [04:26<05:06,  1.56s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0060.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\rOverlaying GON-:  46%|████▌     | 166/360 [04:27<03:50,  1.19s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0136.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Overlaying GON-:  51%|█████▏    | 185/360 [04:56<04:28,  1.53s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0178.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Overlaying GON-:  53%|█████▎    | 191/360 [05:05<04:22,  1.55s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0216.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Overlaying GON-:  54%|█████▍    | 195/360 [05:09<03:39,  1.33s/it]"]},{"output_type":"stream","name":"stdout","text":["⚠️ Skipping unreadable: /content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0106.jpg\n"]},{"output_type":"stream","name":"stderr","text":["Overlaying GON-: 100%|██████████| 360/360 [09:41<00:00,  1.61s/it]\n"]}]},{"cell_type":"code","source":["%%capture\n","!pip install --upgrade git+https://github.com/huggingface/transformers\n","!pip install pillow matplotlib segmentation-models-pytorch\n"],"metadata":{"id":"WtemewuKc4dM","executionInfo":{"status":"ok","timestamp":1752006064633,"user_tz":-330,"elapsed":162879,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoImageProcessor, EomtForUniversalSegmentation\n","\n","processor = AutoImageProcessor.from_pretrained(\"tue-mps/ade20k_semantic_eomt_large_512\")\n","model = EomtForUniversalSegmentation.from_pretrained(\"tue-mps/ade20k_semantic_eomt_large_512\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":217,"referenced_widgets":["c632b8399f0342c294170c9b522ad801","0554b06fcc4d4321abdb96eb5e41b4f2","e6002ffda243468492b99f2b2a3d5112","85dcb3b6018a4ea994b2b67a5d7b7dc5","3422bfd5b31547e795d4ee7d3c61d49d","946d0a3aec74408d9120618b19ff23c3","bc8bae6a844d4bd989089bc634cc7b66","0ccaa7835d6b4604add28b8476afe71c","e2b89ed0cce74b289aff965d3d77d26a","aaaf802dc38e46ed81f1f8aeee0a3a24","b1fb3e2e760842aa8563a15c35a48c91","984605a640e147fe85dbb332c1acaf93","8f0de419ea0b44a693834a9aed495107","3b9ec165e09c41e3b9f0d884813f00ca","7fe4184074b04a839c324901cf625b26","bd6b10cbf96948acac328da9fc27bc19","4d787c6fe13e458981ca2aeab6ad0215","ae96e5aa05d94d5794468e440e398f21","448b9b7127dc43d4a5b4357d4b4f71e4","f8c06cc8a04c4fcf93c1a686280e077a","66c7405b2e874195b452e86a676b6c41","f28691f76c9d4cdc8b1ebf8f81625e62","fe3c4c95830f4e4f9c1ac1c19e8d8046","650d24cba7444f2ea1eec14f0ac88c39","77a61253ea9f427f90431debde33ed74","69fbd53c75cd48ca949c88ac2a1946cb","095954c8412a46c1acc4b9df26ad3132","dc85636566664275b85f27e978503c46","bb40157d02a246ffaa1976a7f14092bd","96c2a8e3d3d54d0cb2fe3856b84c22c8","0a11729aee2540f0a60b8b65112565f0","a87a4c117b254a3f8956fbf64649db32","2055a69cf6a647f98ca4ac61a8731295"]},"id":"VnVYWB-gdLoJ","executionInfo":{"status":"ok","timestamp":1752006108961,"user_tz":-330,"elapsed":44319,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"28832e64-97a0-47d1-e822-06ba460c8765"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c632b8399f0342c294170c9b522ad801"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"984605a640e147fe85dbb332c1acaf93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe3c4c95830f4e4f9c1ac1c19e8d8046"}},"metadata":{}}]},{"cell_type":"markdown","source":["## EOMT classification and segmentation overlays"],"metadata":{"id":"1zuZhqmiZxqB"}},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torchvision import transforms\n","from segmentation_models_pytorch import Unet\n","from transformers import EomtForUniversalSegmentation\n","import torch.nn as nn\n","from PIL import Image\n","\n","# -------------------------------\n","# CONFIGURATION\n","# -------------------------------\n","UNET_PATH = \"/content/drive/MyDrive/glaucoma_detection/cdr_src/unet_efficientnetb4_drishti.pth\"\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/glaucoma_detection/eomt/best_eomt.pth\"\n","TEST_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets\"\n","OUT_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/eomt_predictions.csv\"\n","VISUAL_DIR = \"/content/drive/MyDrive/glaucoma_detection/test_sets/eomt_visuals\"\n","\n","device = torch.device(\"cpu\")\n","RESIZE_DIM = (512, 512)\n","\n","# -------------------------------\n","# LOAD MODELS\n","# -------------------------------\n","print(\"📅 Loading U-Net EfficientNet-B4 on CPU...\")\n","unet = Unet(\n","    encoder_name=\"efficientnet-b4\",\n","    encoder_weights=None,\n","    in_channels=3,\n","    classes=2\n",").to(device)\n","unet.load_state_dict(torch.load(UNET_PATH, map_location=device))\n","unet.eval()\n","\n","print(\"📅 Loading EOMT and classifier head...\")\n","eomt = EomtForUniversalSegmentation.from_pretrained(\"tue-mps/ade20k_semantic_eomt_large_512\").to(device)\n","\n","class ClassifierHead(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 2)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","classifier_head = ClassifierHead().to(device)\n","\n","ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n","eomt.load_state_dict(ckpt['model_state_dict'])\n","classifier_head.load_state_dict(ckpt['classifier_state_dict'])\n","eomt.eval()\n","classifier_head.eval()\n","\n","normalize = transforms.Normalize([0.485, 0.456, 0.406],\n","                                 [0.229, 0.224, 0.225])\n","\n","# -------------------------------\n","# OD/OC Prediction Function\n","# -------------------------------\n","@torch.no_grad()\n","def predict_od_oc(img_np):\n","    tensor = transforms.ToTensor()(cv2.resize(img_np, RESIZE_DIM)).unsqueeze(0).to(device)\n","    tensor = normalize(tensor[0]).unsqueeze(0)\n","    output = unet(tensor)\n","    probs = torch.sigmoid(output).squeeze().cpu().numpy()\n","    return probs[0] > 0.5, probs[1] > 0.5  # disc, cup\n","\n","# -------------------------------\n","# EOMT Classification Function\n","# -------------------------------\n","@torch.no_grad()\n","def classify_eomt(fundus_rgb):\n","    fundus_rgb = cv2.resize(fundus_rgb, RESIZE_DIM)\n","    fundus_tensor = transforms.ToTensor()(fundus_rgb)\n","    fundus_tensor = normalize(fundus_tensor).unsqueeze(0).to(device)\n","    outputs = eomt(pixel_values=fundus_tensor, output_hidden_states=True)\n","    features = outputs.hidden_states[-1].mean(dim=1)\n","    logits = classifier_head(features)\n","    probs = torch.softmax(logits, dim=1).squeeze().cpu().numpy()\n","    pred = int(np.argmax(probs))\n","    return pred, probs[1]  # pred, probability of GON+\n","\n","# -------------------------------\n","# Overlay Visualization Function\n","# -------------------------------\n","def overlay_masks(image, disc_mask, cup_mask, label):\n","    overlay = image.copy()\n","    disc_mask = cv2.resize(disc_mask.astype(np.uint8), image.shape[:2][::-1])\n","    cup_mask = cv2.resize(cup_mask.astype(np.uint8), image.shape[:2][::-1])\n","    overlay[cup_mask == 1] = [0, 0, 255]    # Red = cup\n","    overlay[disc_mask == 1] = [0, 255, 0]   # Green = disc\n","    return cv2.putText(overlay, f\"GON: {'+' if label else '-'}\", (10, 30),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n","\n","# -------------------------------\n","# Inference Loop\n","# -------------------------------\n","results = []\n","\n","for label_dir in [\"GON+\", \"GON-\"]:\n","    label_folder = os.path.join(TEST_ROOT, label_dir)\n","    out_folder = os.path.join(VISUAL_DIR, label_dir)\n","    os.makedirs(out_folder, exist_ok=True)\n","    true_label = 1 if label_dir == \"GON+\" else 0\n","\n","    for fname in tqdm(os.listdir(label_folder), desc=f\"Classifying {label_dir}\"):\n","        try:\n","            img_path = os.path.join(label_folder, fname)\n","            fundus = cv2.imread(img_path)\n","            if fundus is None:\n","                print(f\"⚠️ Could not read: {img_path}\")\n","                continue\n","\n","            disc_mask, cup_mask = predict_od_oc(fundus)\n","            pred_label, pred_prob = classify_eomt(fundus)\n","\n","            results.append({\n","                \"image_path\": img_path,\n","                \"true_label\": true_label,\n","                \"predicted_label\": pred_label,\n","                \"predicted_probability\": pred_prob\n","            })\n","\n","            vis = overlay_masks(cv2.resize(fundus, RESIZE_DIM), disc_mask, cup_mask, pred_label)\n","            out_path = os.path.join(out_folder, fname.replace(\".jpg\", \"_eomt.png\"))\n","            cv2.imwrite(out_path, vis)\n","\n","        except Exception as e:\n","            print(f\"❌ Error on {fname}: {e}\")\n","\n","# -------------------------------\n","# Save Results CSV\n","# -------------------------------\n","df = pd.DataFrame(results)\n","df.to_csv(OUT_CSV, index=False, encoding='utf-8')\n","print(f\"✅ Saved predictions to: {OUT_CSV}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"Un4RjVfWRMq-","executionInfo":{"status":"error","timestamp":1752006822778,"user_tz":-330,"elapsed":713799,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"4784c78f-caf5-4aa9-bd9d-0c69c0f5cec7"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["📅 Loading U-Net EfficientNet-B4 on CPU...\n","📅 Loading EOMT and classifier head...\n"]},{"output_type":"stream","name":"stderr","text":["Classifying GON+:  10%|▉         | 40/417 [11:16<1:46:12, 16.90s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-5-613138544.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mdisc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcup_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_od_oc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_eomt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             results.append({\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-5-613138544.py\u001b[0m in \u001b[0;36mclassify_eomt\u001b[0;34m(fundus_rgb)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mfundus_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundus_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mfundus_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundus_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meomt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfundus_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/eomt/modeling_eomt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, mask_labels, class_labels, output_hidden_states, output_attentions, patch_offsets)\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/eomt/modeling_eomt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;31m# in Eomt, layernorm is also applied after self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_scale2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/eomt/modeling_eomt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","from torchvision import transforms\n","from segmentation_models_pytorch import Unet\n","from transformers import EomtForUniversalSegmentation\n","import torch.nn as nn\n","from PIL import Image\n","\n","# -------------------------------\n","# CONFIGURATION\n","# -------------------------------\n","UNET_PATH = \"/content/drive/MyDrive/glaucoma_detection/cdr_src/unet_efficientnetb4_drishti.pth\"\n","CHECKPOINT_PATH = \"/content/drive/MyDrive/glaucoma_detection/eomt/best_eomt.pth\"\n","TEST_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets\"\n","OUT_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/eomt_predictions.csv\"\n","VISUAL_DIR = \"/content/drive/MyDrive/glaucoma_detection/test_sets/eomt_visuals\"\n","\n","device = torch.device(\"cpu\")\n","RESIZE_DIM = (512, 512)\n","\n","# -------------------------------\n","# LOAD MODELS\n","# -------------------------------\n","print(\"📅 Loading U-Net EfficientNet-B4 on CPU...\")\n","unet = Unet(\n","    encoder_name=\"efficientnet-b4\",\n","    encoder_weights=None,\n","    in_channels=3,\n","    classes=2\n",").to(device)\n","unet.load_state_dict(torch.load(UNET_PATH, map_location=device))\n","unet.eval()\n","\n","print(\"📅 Loading EOMT and classifier head...\")\n","eomt = EomtForUniversalSegmentation.from_pretrained(\"tue-mps/ade20k_semantic_eomt_large_512\").to(device)\n","\n","class ClassifierHead(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(512, 2)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","classifier_head = ClassifierHead().to(device)\n","\n","ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n","eomt.load_state_dict(ckpt['model_state_dict'])\n","classifier_head.load_state_dict(ckpt['classifier_state_dict'])\n","eomt.eval()\n","classifier_head.eval()\n","\n","normalize = transforms.Normalize([0.485, 0.456, 0.406],\n","                                 [0.229, 0.224, 0.225])\n","\n","# -------------------------------\n","# OD/OC Prediction Function\n","# -------------------------------\n","@torch.no_grad()\n","def predict_od_oc(img_np):\n","    tensor = transforms.ToTensor()(cv2.resize(img_np, RESIZE_DIM)).unsqueeze(0).to(device)\n","    tensor = normalize(tensor[0]).unsqueeze(0)\n","    output = unet(tensor)\n","    probs = torch.sigmoid(output).squeeze().cpu().numpy()\n","    return probs[0] > 0.5, probs[1] > 0.5  # disc, cup\n","\n","# -------------------------------\n","# EOMT Classification Function\n","# -------------------------------\n","@torch.no_grad()\n","def classify_eomt(fundus_rgb):\n","    fundus_rgb = cv2.resize(fundus_rgb, RESIZE_DIM)\n","    fundus_tensor = transforms.ToTensor()(fundus_rgb)\n","    fundus_tensor = normalize(fundus_tensor).unsqueeze(0).to(device)\n","    outputs = eomt(pixel_values=fundus_tensor, output_hidden_states=True)\n","    features = outputs.hidden_states[-1].mean(dim=1)\n","    logits = classifier_head(features)\n","    pred = torch.argmax(logits, dim=1).item()\n","    return pred\n","\n","# -------------------------------\n","# Overlay Visualization Function\n","# -------------------------------\n","def overlay_masks(image, disc_mask, cup_mask, label):\n","    overlay = image.copy()\n","    disc_mask = cv2.resize(disc_mask.astype(np.uint8), image.shape[:2][::-1])\n","    cup_mask = cv2.resize(cup_mask.astype(np.uint8), image.shape[:2][::-1])\n","    overlay[cup_mask == 1] = [0, 0, 255]    # Red = cup\n","    overlay[disc_mask == 1] = [0, 255, 0]   # Green = disc\n","    return cv2.putText(overlay, f\"GON: {'+' if label else '-'}\", (10, 30),\n","                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n","\n","# -------------------------------\n","# Load Results and Compute Metrics\n","# -------------------------------\n","df = pd.read_csv(OUT_CSV)\n","y_true = df['true_label']\n","y_pred = df['predicted_label']\n","\n","print(\"\\n📊 Classification Report:\")\n","print(classification_report(y_true, y_pred, target_names=[\"GON-\", \"GON+\"]))\n","\n","print(\"\\n🧮 Confusion Matrix:\")\n","print(confusion_matrix(y_true, y_pred))\n","\n","try:\n","    print(\"\\n📈 ROC-AUC Score:\")\n","    print(roc_auc_score(y_true, y_pred))\n","except:\n","    print(\"ROC-AUC Score could not be computed (requires binary probabilities or continuous scores).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e_FJ-DzJ6W73","executionInfo":{"status":"ok","timestamp":1751992931327,"user_tz":-330,"elapsed":18570,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"b3d87759-a168-4ee8-b030-1057561f2d6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["📅 Loading U-Net EfficientNet-B4 on CPU...\n","📅 Loading EOMT and classifier head...\n","\n","📊 Classification Report:\n","              precision    recall  f1-score   support\n","\n","        GON-       0.92      0.22      0.35       353\n","        GON+       0.08      0.78      0.15        32\n","\n","    accuracy                           0.26       385\n","   macro avg       0.50      0.50      0.25       385\n","weighted avg       0.85      0.26      0.33       385\n","\n","\n","🧮 Confusion Matrix:\n","[[ 76 277]\n"," [  7  25]]\n","\n","📈 ROC-AUC Score:\n","0.4982737252124646\n"]}]},{"cell_type":"markdown","source":["## Dinov2 classification using soft fusion within the four classifiers"],"metadata":{"id":"R3a4-SsWZhXT"}},{"cell_type":"code","source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torchvision import transforms\n","from PIL import Image\n","import torch.nn as nn\n","\n","# --------------------------------\n","# CONFIG\n","# --------------------------------\n","DINO_MODEL_PATHS = {\n","    \"drishti\": \"/content/drive/MyDrive/glaucoma_detection/dinov2_classifiers/best_dinov2_classifier_drishti_augmented.pth\",\n","    \"refuge\": \"/content/drive/MyDrive/glaucoma_detection/dinov2_classifiers/best_dinov2_classifier_refuge_only.pth\",\n","    \"eyepacs-airogsv2\":   \"/content/drive/MyDrive/glaucoma_detection/dinov2_classifiers/best_dinov2_classifier_airogs.pth\",\n","    \"hygd\":    \"/content/drive/MyDrive/glaucoma_detection/dinov2_classifiers/best_dinov2_classifier_hygd.pth\",\n","}\n","TEST_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets\"\n","DINO_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/dinov2_predictions.csv\"\n","\n","device = torch.device(\"cpu\")\n","RESIZE_DIM = (518, 518)  # must be divisible by 14 for ViT-S/14\n","NUM_CLASSES = 2\n","\n","# --------------------------------\n","# Load DINOv2 Backbone\n","# --------------------------------\n","dinov2 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\").to(device)\n","dinov2.eval()\n","\n","# Normalize\n","normalize = transforms.Normalize([0.485, 0.456, 0.406],\n","                                 [0.229, 0.224, 0.225])\n","\n","# --------------------------------\n","# Classifier Head Definition\n","# --------------------------------\n","class ClassifierHead(nn.Module):\n","    def __init__(self, in_dim=384, num_classes=2):\n","        super().__init__()\n","        self.fc = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(in_dim, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","# --------------------------------\n","# Load all DINOv2 classifiers\n","# --------------------------------\n","dino_heads = []\n","for _, path in DINO_MODEL_PATHS.items():\n","    clf = ClassifierHead().to(device)\n","    state = torch.load(path, map_location=device)\n","    clf.load_state_dict(state['classifier_state_dict'])\n","    clf.eval()\n","    dino_heads.append(clf)\n","\n","# --------------------------------\n","# DINOv2 Soft-Fused Prediction Function\n","# --------------------------------\n","@torch.no_grad()\n","def classify_dino(fundus_rgb):\n","    fundus_rgb = cv2.resize(fundus_rgb, RESIZE_DIM)\n","    tensor = transforms.ToTensor()(fundus_rgb)\n","    tensor = normalize(tensor).unsqueeze(0).to(device)\n","\n","    features = dinov2(tensor).squeeze()\n","    logits_sum = torch.zeros(NUM_CLASSES).to(device)\n","\n","    for clf in dino_heads:\n","        logits = clf(features.unsqueeze(0))\n","        probs = torch.softmax(logits, dim=1).squeeze()\n","        logits_sum += probs\n","\n","    final_probs = logits_sum / len(dino_heads)\n","    pred = int(torch.argmax(final_probs))\n","    return pred, final_probs[1].item()  # prediction, GON+ probability\n","\n","# --------------------------------\n","# Inference Loop\n","# --------------------------------\n","results = []\n","\n","for label_dir in [\"GON+\", \"GON-\"]:\n","    label_folder = os.path.join(TEST_ROOT, label_dir)\n","    true_label = 1 if label_dir == \"GON+\" else 0\n","\n","    for fname in tqdm(os.listdir(label_folder), desc=f\"Classifying {label_dir}\"):\n","        try:\n","            img_path = os.path.join(label_folder, fname)\n","            fundus = cv2.imread(img_path)\n","            if fundus is None:\n","                print(f\"⚠️ Could not read: {img_path}\")\n","                continue\n","\n","            pred_label, prob = classify_dino(fundus)\n","\n","            results.append({\n","                \"image_path\": img_path,\n","                \"true_label\": true_label,\n","                \"predicted_label\": pred_label,\n","                \"predicted_probability\": prob\n","            })\n","\n","        except Exception as e:\n","            print(f\"❌ Error on {fname}: {e}\")\n","\n","# --------------------------------\n","# Save Results CSV\n","# --------------------------------\n","df = pd.DataFrame(results)\n","df.to_csv(DINO_CSV, index=False, encoding='utf-8')\n","print(f\"✅ Saved predictions to: {DINO_CSV}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"ZBSPO27Md94q","executionInfo":{"status":"error","timestamp":1752003364524,"user_tz":-330,"elapsed":3215366,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"7d26a7ee-f1da-45ce-9210-f36227584974"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["📅 Loading U-Net EfficientNet-B4 on CPU...\n","📅 Loading EOMT and classifier head...\n"]},{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n","Classifying GON-: 100%|██████████| 360/360 [2:24:43<00:00, 24.12s/it]\n","Classifying GON+:   0%|          | 0/417 [00:14<?, ?it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-10-3610209324.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0md_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_od_oc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_eomt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_dino\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mfinal_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-6-2829733724.py\u001b[0m in \u001b[0;36mclassify_eomt\u001b[0;34m(fundus_rgb)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mfundus_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundus_rgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mfundus_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundus_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meomt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfundus_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/eomt/modeling_eomt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, mask_labels, class_labels, output_hidden_states, output_attentions, patch_offsets)\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m             \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/eomt/modeling_eomt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0;31m# in Eomt, layernorm is also applied after self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_scale2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/eomt/modeling_eomt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## DYNAMIC FUSION FOR FINAL PREDICTION USING PROBABLITIES FROM eomt AND dino"],"metadata":{"id":"lxZa8d76ZSj4"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# === CONFIG ===\n","INPUT_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/probability_preds_combined.csv\"\n","FUSED_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/final_dynamic_fusion_predictions.csv\"\n","BENCHMARK_PLOT = \"/content/drive/MyDrive/glaucoma_detection/glaucfusion_dynamic_fusion_benchmark.png\"\n","\n","# === Load prediction probabilities ===\n","df = pd.read_csv(INPUT_CSV)\n","\n","# Columns must include:\n","# 'image', 'true_label', 'eomt_prob_GON+', 'dino_prob_GON+'\n","assert all(col in df.columns for col in ['eomt_prob_GON+', 'dino_prob_GON+', 'true_label'])\n","\n","# === Dynamic Weighting Function ===\n","def dynamic_fusion(p_eomt, p_dino, alpha=1.5):\n","    \"\"\"\n","    Fuse EoMT and DINOv2 predictions dynamically using confidence.\n","    Higher confidence gets higher weight dynamically.\n","    alpha: controls sharpness of weighting.\n","    \"\"\"\n","    conf_eomt = abs(p_eomt - 0.5)\n","    conf_dino = abs(p_dino - 0.5)\n","\n","    # Normalize confidences\n","    w_eomt = conf_eomt ** alpha\n","    w_dino = conf_dino ** alpha\n","    total = w_eomt + w_dino\n","\n","    if total == 0:  # Avoid divide-by-zero\n","        return 0.5\n","    return (w_eomt * p_eomt + w_dino * p_dino) / total\n","\n","# === Apply Dynamic Fusion ===\n","df['final_prob_GON+'] = df.apply(lambda row: dynamic_fusion(row['eomt_prob_GON+'], row['dino_prob_GON+']), axis=1)\n","df['final_pred'] = (df['final_prob_GON+'] >= 0.5).astype(int)\n","\n","# === Save Final Predictions ===\n","df[['image', 'true_label', 'final_pred', 'final_prob_GON+']].to_csv(FUSED_CSV, index=False)\n","print(f\"✅ Saved final dynamic fusion predictions to: {FUSED_CSV}\")\n","\n","# === Evaluate Performance ===\n","y_true = df['true_label']\n","y_pred = df['final_pred']\n","y_score = df['final_prob_GON+']\n","\n","print(\"\\n📊 Classification Report:\")\n","print(classification_report(y_true, y_pred, target_names=['GON-', 'GON+']))\n","\n","print(\"\\n🧮 Confusion Matrix:\")\n","print(confusion_matrix(y_true, y_pred))\n","\n","print(\"\\n📈 ROC-AUC Score:\")\n","print(roc_auc_score(y_true, y_score))\n","\n","# === Benchmark Barplot ===\n","metrics = {\n","    \"Accuracy\": accuracy_score(y_true, y_pred),\n","    \"Precision\": precision_score(y_true, y_pred),\n","    \"Recall\": recall_score(y_true, y_pred),\n","    \"F1-score\": f1_score(y_true, y_pred)\n","}\n","\n","plt.figure(figsize=(8, 5))\n","sns.barplot(x=list(metrics.keys()), y=list(metrics.values()), palette='coolwarm')\n","plt.ylim(0, 1)\n","plt.title(\"📊 Performance Metrics - GlaucFusion (Dynamic Fusion)\")\n","for i, v in enumerate(metrics.values()):\n","    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n","plt.tight_layout()\n","plt.savefig(BENCHMARK_PLOT)\n","plt.show()\n"],"metadata":{"id":"tyHFPft2_8ZL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## dataset class prediction using finetuned resnet-18"],"metadata":{"id":"FA2SEMhxbMNM"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision.models import resnet18, ResNet18_Weights\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","import pandas as pd\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- CONFIGURATION ---\n","RESNET18_MODEL_PATH = \"/content/drive/MyDrive/glaucoma_detection/resnet18_classifier/dataset_classifier_resnet18.pth\"\n","TEST_ROOT = \"/content/drive/MyDrive/glaucoma_detection/test_sets\"\n","OUT_CSV = \"/content/drive/MyDrive/glaucoma_detection/test_sets/resnet18_dataset_predictions.csv\"\n","DIST_PLOT_PATH = \"/content/drive/MyDrive/glaucoma_detection/resnet18_dataset_distribution.png\"\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","DATASET_CLASSES = [\"DRISHTI\", \"EYE-PACS\", \"HYGD\", \"REFUGE\"]  # Make sure order matches training\n","\n","# --- DEFINE MODEL ---\n","class DatasetClassifier(nn.Module):\n","    def __init__(self, num_classes=4):\n","        super(DatasetClassifier, self).__init__()\n","        self.model = resnet18(weights=ResNet18_Weights.DEFAULT)\n","        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# --- LOAD MODEL ---\n","model = DatasetClassifier(num_classes=4).to(DEVICE)\n","model.load_state_dict(torch.load(RESNET18_MODEL_PATH, map_location=DEVICE))\n","model.eval()\n","\n","# --- TRANSFORM ---\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std =[0.229, 0.224, 0.225])\n","])\n","\n","# --- PREDICTION FUNCTION ---\n","@torch.no_grad()\n","def predict_dataset_class(image_path):\n","    img = Image.open(image_path).convert(\"RGB\")\n","    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n","    logits = model(img_tensor)\n","    pred_idx = torch.argmax(logits, dim=1).item()\n","    confidence = torch.softmax(logits, dim=1).squeeze()[pred_idx].item()\n","    return DATASET_CLASSES[pred_idx], confidence\n","\n","# --- INFERENCE LOOP ---\n","results = []\n","for label_folder in [\"GON-\", \"GON+\"]:\n","    folder_path = os.path.join(TEST_ROOT, label_folder)\n","    for fname in tqdm(os.listdir(folder_path), desc=f\"Classifying dataset source for {label_folder}\"):\n","        img_path = os.path.join(folder_path, fname)\n","        try:\n","            pred_class, conf = predict_dataset_class(img_path)\n","            results.append({\n","                \"image\": fname,\n","                \"image_path\": img_path,\n","                \"true_label\": 1 if label_folder == \"GON+\" else 0,\n","                \"predicted_dataset\": pred_class,\n","                \"confidence\": round(conf, 4)\n","            })\n","        except Exception as e:\n","            print(f\"❌ Error on {fname}: {e}\")\n","\n","# --- SAVE RESULTS ---\n","df = pd.DataFrame(results)\n","df.to_csv(OUT_CSV, index=False)\n","print(f\"✅ Saved dataset classification results to: {OUT_CSV}\")\n","\n","# --- VISUALIZATION ---\n","plt.figure(figsize=(8, 5))\n","sns.countplot(data=df, x=\"predicted_dataset\", order=DATASET_CLASSES, palette=\"Spectral\")\n","plt.title(\"📊 Predicted Dataset Distribution (ResNet18)\")\n","plt.xlabel(\"Dataset\")\n","plt.ylabel(\"Image Count\")\n","plt.tight_layout()\n","plt.savefig(DIST_PLOT_PATH)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":892},"id":"aVD0TGjkbWP0","executionInfo":{"status":"ok","timestamp":1752003939646,"user_tz":-330,"elapsed":136820,"user":{"displayName":"123ME0031 Piyush Gupta","userId":"00849504273628911490"}},"outputId":"0642fbff-0c82-4835-f27a-8c2cb0ef0463"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Classifying dataset source for GON-:   1%|          | 3/360 [00:01<02:27,  2.42it/s]"]},{"output_type":"stream","name":"stdout","text":["❌ Error on n0166.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0166.jpg'\n"]},{"output_type":"stream","name":"stderr","text":["\rClassifying dataset source for GON-:   1%|          | 4/360 [00:04<09:01,  1.52s/it]"]},{"output_type":"stream","name":"stdout","text":["❌ Error on n0194.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0194.jpg'\n"]},{"output_type":"stream","name":"stderr","text":["Classifying dataset source for GON-:  46%|████▌     | 166/360 [00:38<00:30,  6.31it/s]"]},{"output_type":"stream","name":"stdout","text":["❌ Error on n0060.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0060.jpg'\n","❌ Error on n0136.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0136.jpg'\n"]},{"output_type":"stream","name":"stderr","text":["Classifying dataset source for GON-:  52%|█████▏    | 187/360 [00:42<00:22,  7.54it/s]"]},{"output_type":"stream","name":"stdout","text":["❌ Error on n0178.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0178.jpg'\n"]},{"output_type":"stream","name":"stderr","text":["Classifying dataset source for GON-:  54%|█████▎    | 193/360 [00:43<00:21,  7.82it/s]"]},{"output_type":"stream","name":"stdout","text":["❌ Error on n0216.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0216.jpg'\n"]},{"output_type":"stream","name":"stderr","text":["Classifying dataset source for GON-:  55%|█████▍    | 197/360 [00:43<00:19,  8.30it/s]"]},{"output_type":"stream","name":"stdout","text":["❌ Error on n0106.jpg: cannot identify image file '/content/drive/MyDrive/glaucoma_detection/test_sets/GON-/n0106.jpg'\n"]},{"output_type":"stream","name":"stderr","text":["Classifying dataset source for GON-: 100%|██████████| 360/360 [01:16<00:00,  4.69it/s]\n","Classifying dataset source for GON+: 100%|██████████| 417/417 [00:58<00:00,  7.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["✅ Saved dataset classification results to: /content/drive/MyDrive/glaucoma_detection/test_sets/resnet18_dataset_predictions.csv\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-13-370851755.py:80: FutureWarning: \n","\n","Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n","\n","  sns.countplot(data=df, x=\"predicted_dataset\", order=DATASET_CLASSES, palette=\"Spectral\")\n","/tmp/ipython-input-13-370851755.py:84: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n","  plt.tight_layout()\n","/tmp/ipython-input-13-370851755.py:85: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n","  plt.savefig(DIST_PLOT_PATH)\n","/usr/local/lib/python3.11/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n","  fig.canvas.print_figure(bytes_io, **kw)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 800x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## final output with all meaningfull information"],"metadata":{"id":"Q4FVw2_TbYI1"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# --- Paths ---\n","fusion_csv = \"/content/drive/MyDrive/glaucoma_detection/test_sets/final_predictions_softfusion.csv\"\n","dataset_csv = \"/content/drive/MyDrive/glaucoma_detection/test_sets/resnet18_dataset_predictions.csv\"\n","out_csv = \"/content/drive/MyDrive/glaucoma_detection/test_sets/final_predictions_softfusion_with_dataset.csv\"\n","\n","# --- Load both CSVs ---\n","df_fusion = pd.read_csv(fusion_csv)\n","df_dataset = pd.read_csv(dataset_csv)\n","\n","# --- Merge on image filename ---\n","df_merged = pd.merge(df_fusion, df_dataset, on=\"image\", how=\"left\")\n","\n","# --- Save ---\n","df_merged.to_csv(out_csv, index=False)\n","\n","print(f\"✅ Saved merged CSV with dataset source to:\\n{out_csv}\")\n","print(df_merged.head())\n"],"metadata":{"id":"HxHfs3zIbd76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# --- Load merged CSV ---\n","csv_path = \"/content/drive/MyDrive/glaucoma_detection/test_sets/final_predictions_softfusion_with_dataset.csv\"\n","df = pd.read_csv(csv_path)\n","\n","# --- Prepare metric containers ---\n","dataset_metrics = []\n","\n","# --- Evaluate per predicted dataset ---\n","for ds in df['predicted_dataset'].unique():\n","    subset = df[df['predicted_dataset'] == ds]\n","    y_true = subset['true_label']\n","    y_pred = subset['predicted_label']\n","\n","    acc = accuracy_score(y_true, y_pred)\n","    prec = precision_score(y_true, y_pred, zero_division=0)\n","    rec = recall_score(y_true, y_pred, zero_division=0)\n","    f1 = f1_score(y_true, y_pred, zero_division=0)\n","\n","    try:\n","        roc = roc_auc_score(y_true, y_pred)\n","    except:\n","        roc = None\n","\n","    dataset_metrics.append({\n","        \"Dataset\": ds,\n","        \"Accuracy\": acc,\n","        \"Precision\": prec,\n","        \"Recall\": rec,\n","        \"F1-score\": f1,\n","        \"ROC-AUC\": roc\n","    })\n","\n","# --- Create DataFrame ---\n","metrics_df = pd.DataFrame(dataset_metrics)\n","print(\"📊 Stratified Metrics per Dataset:\")\n","print(metrics_df)\n","\n","# --- Save to CSV (optional) ---\n","metrics_df.to_csv(\"/content/drive/MyDrive/glaucoma_detection/test_sets/stratified_metrics.csv\", index=False)\n","\n","# --- Plot ---\n","plt.figure(figsize=(12, 6))\n","metrics_melted = metrics_df.melt(id_vars=\"Dataset\", value_vars=[\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"])\n","sns.barplot(data=metrics_melted, x=\"Dataset\", y=\"value\", hue=\"variable\")\n","plt.ylim(0, 1)\n","plt.title(\"📈 Per-Dataset GON Prediction Metrics\")\n","plt.ylabel(\"Score\")\n","plt.legend(title=\"Metric\")\n","plt.tight_layout()\n","plt.savefig(\"/content/drive/MyDrive/glaucoma_detection/stratified_metrics_plot.png\")\n","plt.show()\n"],"metadata":{"id":"rpVCdApqb3pl"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c632b8399f0342c294170c9b522ad801":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0554b06fcc4d4321abdb96eb5e41b4f2","IPY_MODEL_e6002ffda243468492b99f2b2a3d5112","IPY_MODEL_85dcb3b6018a4ea994b2b67a5d7b7dc5"],"layout":"IPY_MODEL_3422bfd5b31547e795d4ee7d3c61d49d"}},"0554b06fcc4d4321abdb96eb5e41b4f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_946d0a3aec74408d9120618b19ff23c3","placeholder":"​","style":"IPY_MODEL_bc8bae6a844d4bd989089bc634cc7b66","value":"preprocessor_config.json: 100%"}},"e6002ffda243468492b99f2b2a3d5112":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ccaa7835d6b4604add28b8476afe71c","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2b89ed0cce74b289aff965d3d77d26a","value":665}},"85dcb3b6018a4ea994b2b67a5d7b7dc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aaaf802dc38e46ed81f1f8aeee0a3a24","placeholder":"​","style":"IPY_MODEL_b1fb3e2e760842aa8563a15c35a48c91","value":" 665/665 [00:00&lt;00:00, 24.9kB/s]"}},"3422bfd5b31547e795d4ee7d3c61d49d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"946d0a3aec74408d9120618b19ff23c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc8bae6a844d4bd989089bc634cc7b66":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ccaa7835d6b4604add28b8476afe71c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2b89ed0cce74b289aff965d3d77d26a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aaaf802dc38e46ed81f1f8aeee0a3a24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1fb3e2e760842aa8563a15c35a48c91":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"984605a640e147fe85dbb332c1acaf93":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f0de419ea0b44a693834a9aed495107","IPY_MODEL_3b9ec165e09c41e3b9f0d884813f00ca","IPY_MODEL_7fe4184074b04a839c324901cf625b26"],"layout":"IPY_MODEL_bd6b10cbf96948acac328da9fc27bc19"}},"8f0de419ea0b44a693834a9aed495107":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d787c6fe13e458981ca2aeab6ad0215","placeholder":"​","style":"IPY_MODEL_ae96e5aa05d94d5794468e440e398f21","value":"config.json: "}},"3b9ec165e09c41e3b9f0d884813f00ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_448b9b7127dc43d4a5b4357d4b4f71e4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8c06cc8a04c4fcf93c1a686280e077a","value":1}},"7fe4184074b04a839c324901cf625b26":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66c7405b2e874195b452e86a676b6c41","placeholder":"​","style":"IPY_MODEL_f28691f76c9d4cdc8b1ebf8f81625e62","value":" 6.78k/? [00:00&lt;00:00, 139kB/s]"}},"bd6b10cbf96948acac328da9fc27bc19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d787c6fe13e458981ca2aeab6ad0215":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae96e5aa05d94d5794468e440e398f21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"448b9b7127dc43d4a5b4357d4b4f71e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"f8c06cc8a04c4fcf93c1a686280e077a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66c7405b2e874195b452e86a676b6c41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f28691f76c9d4cdc8b1ebf8f81625e62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe3c4c95830f4e4f9c1ac1c19e8d8046":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_650d24cba7444f2ea1eec14f0ac88c39","IPY_MODEL_77a61253ea9f427f90431debde33ed74","IPY_MODEL_69fbd53c75cd48ca949c88ac2a1946cb"],"layout":"IPY_MODEL_095954c8412a46c1acc4b9df26ad3132"}},"650d24cba7444f2ea1eec14f0ac88c39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc85636566664275b85f27e978503c46","placeholder":"​","style":"IPY_MODEL_bb40157d02a246ffaa1976a7f14092bd","value":"model.safetensors: 100%"}},"77a61253ea9f427f90431debde33ed74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96c2a8e3d3d54d0cb2fe3856b84c22c8","max":1264130192,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a11729aee2540f0a60b8b65112565f0","value":1264130192}},"69fbd53c75cd48ca949c88ac2a1946cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a87a4c117b254a3f8956fbf64649db32","placeholder":"​","style":"IPY_MODEL_2055a69cf6a647f98ca4ac61a8731295","value":" 1.26G/1.26G [00:16&lt;00:00, 128MB/s]"}},"095954c8412a46c1acc4b9df26ad3132":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc85636566664275b85f27e978503c46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb40157d02a246ffaa1976a7f14092bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96c2a8e3d3d54d0cb2fe3856b84c22c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a11729aee2540f0a60b8b65112565f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a87a4c117b254a3f8956fbf64649db32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2055a69cf6a647f98ca4ac61a8731295":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}